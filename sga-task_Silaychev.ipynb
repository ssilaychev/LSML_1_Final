{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks, code quality and readability (no single-letter variables, PEP8 compliant) etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-09-27 20:29:53,147 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "df = se.read.csv(\"hdfs:/data/clickstream.csv\", header=True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register a temporary table for SQL\n",
    "df.createOrReplaceTempView(\"clickstream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_id', 'session_id', 'event_type', 'event_page', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL solution with <=2 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "se.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW ValidRoutes AS\n",
    "    SELECT user_id, session_id, COLLECT_LIST(event_page) AS route\n",
    "    FROM (\n",
    "        SELECT user_id, session_id, event_page, timestamp\n",
    "        FROM clickstream\n",
    "        WHERE (user_id, session_id, timestamp) NOT IN (\n",
    "            SELECT user_id, session_id, timestamp\n",
    "            FROM clickstream\n",
    "            WHERE event_type LIKE '%error%'\n",
    "        )\n",
    "        ORDER BY user_id, session_id, timestamp\n",
    "    )\n",
    "    GROUP BY user_id, session_id\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "result_2q = se.sql(\"\"\"\n",
    "    SELECT CONCAT_WS('-', route) as route, COUNT(*) as count\n",
    "    FROM ValidRoutes\n",
    "    GROUP BY route\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 30\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 2435|\n",
      "|           main-main| 1459|\n",
      "|      main-main-main|  850|\n",
      "| main-main-main-main|  549|\n",
      "|main-main-main-ma...|  321|\n",
      "|main-main-main-ma...|  168|\n",
      "|        main-archive|  153|\n",
      "|         main-rabota|  145|\n",
      "|       main-internet|  122|\n",
      "|main-main-main-ma...|  118|\n",
      "|           main-news|  102|\n",
      "|          main-bonus|   99|\n",
      "|  main-rabota-rabota|   92|\n",
      "|    main-main-rabota|   86|\n",
      "|   main-main-archive|   84|\n",
      "|main-archive-archive|   84|\n",
      "|        main-tariffs|   80|\n",
      "|         main-online|   77|\n",
      "|main-internet-int...|   73|\n",
      "|     main-main-bonus|   73|\n",
      "|          main-vklad|   72|\n",
      "|      main-main-news|   68|\n",
      "|main-main-main-ar...|   65|\n",
      "|    main-bonus-bonus|   62|\n",
      "|main-main-main-ma...|   58|\n",
      "|    main-main-online|   55|\n",
      "|      main-news-news|   53|\n",
      "|main-bonus-bonus-...|   52|\n",
      "|main-main-main-in...|   51|\n",
      "|main-archive-arch...|   51|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_2q.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# result_2q.write.csv(\"hdfs:/data/result_sql_2q_Silaychev.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL solution with 1 query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_1q = se.sql(\"\"\"\n",
    "    WITH CleanSessions AS (\n",
    "        SELECT user_id, session_id, event_page, timestamp\n",
    "        FROM clickstream\n",
    "        WHERE (user_id, session_id, timestamp) NOT IN (\n",
    "            SELECT user_id, session_id, timestamp\n",
    "            FROM clickstream\n",
    "            WHERE event_type LIKE '%error%'\n",
    "        )\n",
    "        ORDER BY user_id, session_id, timestamp\n",
    "    ),\n",
    "    Routes AS (\n",
    "        SELECT user_id, session_id, COLLECT_LIST(event_page) AS route\n",
    "        FROM CleanSessions\n",
    "        GROUP BY user_id, session_id\n",
    "    )\n",
    "    SELECT CONCAT_WS('-', route) as route, COUNT(*) as count\n",
    "    FROM Routes\n",
    "    GROUP BY route\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 30\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "### w/o duplicates\n",
    "# result = spark.sql(\"\"\"\n",
    "#     WITH CleanSessions AS (\n",
    "#         SELECT user_id, session_id, event_page, timestamp\n",
    "#         FROM clickstream\n",
    "#         WHERE (user_id, session_id, timestamp) NOT IN (\n",
    "#             SELECT user_id, session_id, timestamp\n",
    "#             FROM clickstream\n",
    "#             WHERE event_type LIKE '%error%'\n",
    "#         )\n",
    "#         ORDER BY user_id, session_id, timestamp\n",
    "#     ),\n",
    "#     Routes AS (\n",
    "#         SELECT user_id, session_id, ARRAY_DISTINCT(COLLECT_LIST(event_page)) AS route #  <-- in case we want to remove duplicates\n",
    "#         FROM CleanSessions\n",
    "#         GROUP BY user_id, session_id\n",
    "#     )\n",
    "#     SELECT CONCAT_WS('-', route) as route, COUNT(*) as count\n",
    "#     FROM Routes\n",
    "#     GROUP BY route\n",
    "#     ORDER BY count DESC\n",
    "#     LIMIT 30\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 2435|\n",
      "|           main-main| 1459|\n",
      "|      main-main-main|  850|\n",
      "| main-main-main-main|  549|\n",
      "|main-main-main-ma...|  321|\n",
      "|main-main-main-ma...|  168|\n",
      "|        main-archive|  153|\n",
      "|         main-rabota|  145|\n",
      "|       main-internet|  122|\n",
      "|main-main-main-ma...|  118|\n",
      "|           main-news|  102|\n",
      "|          main-bonus|   99|\n",
      "|  main-rabota-rabota|   92|\n",
      "|    main-main-rabota|   86|\n",
      "|   main-main-archive|   84|\n",
      "|main-archive-archive|   84|\n",
      "|        main-tariffs|   80|\n",
      "|         main-online|   77|\n",
      "|main-internet-int...|   73|\n",
      "|     main-main-bonus|   73|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_1q.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_1q.write.csv(\"hdfs:/data/result_sql_1q_Silaychev.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complexity evaluation for Spark SQL solutions.\n",
    "- Solution with 1 query:\n",
    "\n",
    "1. Filtering corrupted sessions (using the NOT IN clause): This operation's complexity is *O(N)*, where N is the number of rows in the dataset.\n",
    "\n",
    "2. Grouping by user_id and session_id and collecting lists: This is also a *O(N)* operation, as we're iterating through each row in the dataset.\n",
    "\n",
    "3. Aggregating based on the route and counting: In the worst-case scenario, this can be O(N) if every session has a unique route.\n",
    "\n",
    "Overall, the SQL solution is *O(N)*.\n",
    "\n",
    "- Solution with 2 queries:\n",
    "\n",
    "The complexity remains *O(N)* for similar reasons as mentioned in the 1-query solution. Breaking the solution into two queries doesn't change the overall complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark RDD solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading and setting up the RDD\n",
    "rdd = sc.textFile(\"hdfs:/data/clickstream.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "header = rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd = rdd.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the rows based on the comma\n",
    "rdd_split = rdd.map(lambda line: line.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to identify rows with errors\n",
    "def has_error(row):\n",
    "    return 'error' in row[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter sessions with errors\n",
    "filtered_rdd = rdd_split.filter(lambda row: not has_error(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract and build routes\n",
    "routes = filtered_rdd.map(lambda row: ((row[0], row[1]), [row[3]])).reduceByKey(lambda a, b: a + b).map(lambda row: (\"-\".join(row[1]), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count routes\n",
    "route_counts = routes.reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IN CASE OF DISTINCT ROUTES --- to avoid duplicates\n",
    "\n",
    "# ... [Initial setup remains the same]\n",
    "\n",
    "# # Extract and build distinct routes\n",
    "# routes = filtered_rdd.map(lambda row: ((row[0], row[1]), [row[3]])).reduceByKey(lambda a, b: list(set(a + b))).map(lambda row: (\"-\".join(row[1]), 1))\n",
    "\n",
    "# # Count routes\n",
    "# route_counts = routes.reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_counts.take(30).saveAsTextFile(\"hdfs:/data/result_rdd_Silaychev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('main', 2435),\n",
       " ('main-main', 1459),\n",
       " ('main-main-main', 850),\n",
       " ('main-main-main-main', 549),\n",
       " ('main-main-main-main-main', 321),\n",
       " ('main-main-main-main-main-main', 168),\n",
       " ('main-archive', 153),\n",
       " ('main-rabota', 145),\n",
       " ('main-internet', 122),\n",
       " ('main-main-main-main-main-main-main', 118),\n",
       " ('main-news', 102),\n",
       " ('main-bonus', 99),\n",
       " ('main-rabota-rabota', 92),\n",
       " ('main-main-rabota', 86),\n",
       " ('main-archive-archive', 84),\n",
       " ('main-main-archive', 84),\n",
       " ('main-tariffs', 80),\n",
       " ('main-online', 76),\n",
       " ('main-main-bonus', 74),\n",
       " ('main-internet-internet', 73),\n",
       " ('main-vklad', 72),\n",
       " ('main-main-news', 68),\n",
       " ('main-main-main-archive', 65),\n",
       " ('main-bonus-bonus', 62),\n",
       " ('main-main-main-main-main-main-main-main', 58),\n",
       " ('main-main-online', 55),\n",
       " ('main-news-news', 53),\n",
       " ('main-bonus-bonus-bonus', 52),\n",
       " ('main-archive-archive-archive', 51),\n",
       " ('main-main-main-internet', 51)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show top 30 results\n",
    "route_counts.take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complexity evaluation for Spark RDD solution\n",
    "\n",
    "\n",
    "1. Filtering rows: Complexity is *O(N)*.\n",
    "2. Building routes using reduceByKey: Complexity is *O(N)*. This is because reduceByKey performs a distributed aggregation.\n",
    "3. Counting routes using reduceByKey: Again, this is *O(N)* in the worst-case scenario if each route is unique.\n",
    "4. Sorting routes based on count: Sorting complexity is *O(N log N)*.\n",
    "5. Considering all these steps, the RDD solution has a complexity of *O(N log N)* due to the sorting step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DF Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the clickstream.csv data into a DataFrame\n",
    "df = se.read.csv(\"hdfs:/data/clickstream.csv\", header=True, sep=\"\\t\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove corrupted sessions\n",
    "sessions_without_errors = df.withColumn(\"has_error\", F.col(\"event_type\").like(\"%error%\")).filter(~F.col(\"has_error\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create routes\n",
    "routes_df = sessions_without_errors.groupBy(\"user_id\", \"session_id\").agg(F.collect_list(\"event_page\").alias(\"route\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the routes\n",
    "result_df = routes_df.groupBy(\"route\").count().orderBy(F.desc(\"count\")).limit(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To avoid duplicates\n",
    "\n",
    "\n",
    "### ... [Initial setup remains the same]\n",
    "\n",
    "# # Create routes with distinct event pages\n",
    "# distinct_routes_df = sessions_without_errors.groupBy(\"user_id\", \"session_id\").agg(F.array_distinct(F.collect_list(\"event_page\")).alias(\"route\"))\n",
    "\n",
    "# # Count the distinct routes\n",
    "# result_df = distinct_routes_df.groupBy(\"route\").count().orderBy(F.desc(\"count\")).limit(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df.write.csv(\"hdfs:/data/result_df_Silaychev.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|              [main]| 2435|\n",
      "|        [main, main]| 1459|\n",
      "|  [main, main, main]|  850|\n",
      "|[main, main, main...|  549|\n",
      "|[main, main, main...|  321|\n",
      "|[main, main, main...|  168|\n",
      "|     [main, archive]|  152|\n",
      "|      [main, rabota]|  144|\n",
      "|    [main, internet]|  122|\n",
      "|[main, main, main...|  118|\n",
      "|        [main, news]|  102|\n",
      "|       [main, bonus]|   99|\n",
      "|[main, rabota, ra...|   92|\n",
      "|[main, main, rabota]|   86|\n",
      "|[main, main, arch...|   84|\n",
      "|[main, archive, a...|   84|\n",
      "|     [main, tariffs]|   80|\n",
      "|      [main, online]|   76|\n",
      "| [main, main, bonus]|   74|\n",
      "|[main, internet, ...|   73|\n",
      "|       [main, vklad]|   72|\n",
      "|  [main, main, news]|   68|\n",
      "|[main, main, main...|   64|\n",
      "|[main, bonus, bonus]|   62|\n",
      "|[main, main, main...|   58|\n",
      "|[main, main, online]|   55|\n",
      "|  [main, news, news]|   53|\n",
      "|[main, bonus, bon...|   51|\n",
      "|[main, main, main...|   51|\n",
      "|[main, archive, a...|   50|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "result_df.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complexity evaluation for Spark DF solution\n",
    "\n",
    "\n",
    "1. Filtering corrupted sessions: O(N) complexity.\n",
    "2. Grouping and creating routes: O(N) complexity.\n",
    "3. Aggregating based on the route and counting: O(N) complexity.\n",
    "\n",
    "The Spark DataFrame solution has a complexity of O(N)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing user behavior through clickstreams or similar logging mechanisms, slight differences in outputs can arise due to various reasons, even if the core logic of the solution remains consistent. \n",
    "\n",
    "Here are a few reasons why outputs might have slight differences, that I came up with:\n",
    "\n",
    "- Non-deterministic Aggregation: Some operations, especially when parallelized over many nodes in a distributed system like Spark, might not always return results in the same exact order. For example, when you're collecting pages into a list, the order might differ between runs. However, when I checked, we had only one node. Thus, this reason is not applicable.\n",
    "\n",
    "- Error Handling: Different approaches might handle errors differently. For example, if a user session has an error event, one approach might discard the entire session while another might only discard the events after the error. I tried to ensure the consistency, however it is hard to have case-by-case view on the matter.\n",
    "\n",
    "- Intermediate Steps: Even if two approaches aim to achieve the same final result, they might have different intermediate steps that can lead to slight differences in the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
